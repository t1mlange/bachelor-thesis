    Performance Evaluation

    In the last chapter, we have shown that our implementation has the necessary soundness to be viable and yields the expected results. 

    We now evaluate our implementation against the existing implementation in FlowDroid.

    

    DroidBench

    We already introduced Droidbench in s:droidbenchvalidation to validate the soundness of our backward-directed implementation. 

    In this section, we focus on the performance in comparison to the existing forward-directed implementation in FlowDroid. 

    DroidBench has the advantage that all apps are crafted explicitly for benchmarking taint analysis. 

    So, most tests only contain a single-figure number of sources and sinks. 

    Also, the number of sources and sinks are often equal or differ by one to test whether the tool can differentiate something. 

    These simplify the comparison between both analysis directions as neither one has an initial disadvantage.

    Most test cases are small enough to be analyzed in sub-two seconds on an average four-core desktop CPU from 2012. 

    Our test environment is not isolated, so background tasks and the process scheduler can affect the runtime. 

    The short runtime, together with the variance of the unisolated testing environment, render the runtime unusable as a comparison point. 

    In contrast, edge propagations are deterministic(This is only true if there are enough resources. FlowDroid tries to gracefully terminate when running low on memory. Also, timeouts result in a non-reproducible number of edge propagations.) and correlate with the runtime. 

    Thus, we only use the number of propagations to compare both implementations.

    The configuration is the same as described in s:droidbenchconfig.

    Results

    We compare all test cases where both implementations yield the same result. 

    When rows only contain hyphens, either the result of the test case differed between the two analyses or the IFDS analysis did not start, e.g., because no sink is in the reachable code. 

    I denotes the number of edge propagations inside the infoflow analysis and A the number of edge propagations inside the alias analysis. 

    We calculated the absolute difference as . 

    The relative difference calculates as follows: . 

    Hence, negative values signify the backward analysis performed better. 

    The full results are in t:droidbenchevaluation.

    In general, both implementations have similar average edge propagation counts. 

    There are not many test cases where both perform identically; instead, dependent on the specific test case, the relative difference is between  and . 

    So, the expected behavior from s:complexity occurred: it highly depends on the analyzed app.

    However, we did not expect cases that let the backward edge propagations explode up to a factor of , as seen in LifecycleTest#BroadcastReceiverLifecycle3 and others. 

    In contrast, the existing forward implementation only at most a relative difference of .

            

    

    Result Explanation

    We define tests with a relative difference greater than  as worth investigating. 

    In the following, we explain why our implementation performed worse than expected.

    PrivateDataLeak3 

    This test contains two sinks and one source. 

    The tainted data is written to a file, later read from the file and then leaked. 

    FlowDroid does not support tracking taints over files, so it only finds a leak from source to file write but misses the leak from file read to send SMS. 

    Due to EasyTaintWrapper's simplicity, overtainting happens in the backward direction. When FileInputStream fis = openFileInput("out.txt"); is called with fis tainted, EasyTaintWrapper also taints the base object - the MainActivity in this case. 

    As the MainActivity has a enourmous scope, the taint has a long lifetime and many other taints could derive from this taint. 

    This taint explains the relative difference of .

    Using the more precise SummaryTaintWrapper, the edges reduce to  and a relative difference of , which is more reasonable. 

    It is still higher because of the second sink. 

    MultiHandlers1

    Two LocationListeners are registered in different activities. 

    In both activities, an instance field is a parameter of a sink.

    So there are two possible paths where something could be leaked. 

    The LocationListener does not call any source on the first path, while the second path has an empty setter method killing the taint.

    For the first path, the backward analysis has to propagate the taint into the LocationListener to notice that this is a dead-end while the forward's search does not even start there.

    For the second path, the backward analysis seems to suffer because it starts at an instance field taint with a larger scope than a local variable.

    BroadcastReceiverLifecycle3

    The test contains five sinks but only one source. 

    If we only consider the leak path, both implementations perform equally. 

    The four other sinks are responsible for the overhead on edge propagations.

    

    Reflection6

    The reflective call site has multiple callees in the interprocedural control-flow graph. 

    Backward all of these callees are visited, of which only one contains a source statement. 

    Forwards, the taint is introduced in the callee at the source and just one return site needs to be processed.

    Using A More Precise Taint Wrapper

    We noticed the overtainting in PrivateDataLeak3 is caused by the EasyTaintWrapper, thus we now compare how using the SummaryTaintWrapper lowers the edge propagations.

    The full results are in t:droidbenchevaluation_sum. In the table, we compare the EasyTaintWrapper with the SummaryTaintWrapper on our implementation. 

    

    As we already described, PrivateDataLeak3 benefits from the more precise taint wrapper. Similary, many other test cases also benefit. In the end, the average difference does not reflect this because the more precise taint wrapper also produces taints the EasyTaintWrapper missed. The best example is Serialization1, on which only with the SummaryTaintWrapper the leak is found. Altogether, the SummaryTaintWrapper should be the default choice for real-world application because it is more precise without compromising the average edge propagations.

        

    

    Real World Apps

    Configuration

    Our test machine is equipped with four Intel Xeon E5-4650 and 1 TB of RAM. 

    We limited the JVM to 50 GB RAM and FlowDroid on 16 threads per instance. 

    We ran at most four instances in parallel to ensure a one-to-one mapping between CPU threads and FlowDroid threads. 

    Note that the test machine is a shared system, but we made sure there are always enough resources for our evaluation available. 

    Still, background services might influence the performance of a single run. To stamp out this factor, we ran each app three times with a distance of time.

    The time distance between each run is at least the elapsed time from the analysis of the remaining 199 apps.

    If there were outliers(Outliers are runs with at least  difference to the median run and a minimum of  seconds absolute difference.), we repeated the runs.

    We also measured the memory usage of both implementations. 

    Using the memory amount reported by the JVM is not precise because the JVM prefers to take up free memory before running the garbage collector. 

    We borrowed the memory evaluation tool from CleanDroid which internally depends on a memory calculation tool from Twitter(https://mvnrepository.com/artifact/com.twitter.common/objectsize). 

    The memory evaluation tool measures the size of the exploded supergraph in 15 seconds intervals. 

    Because we do not want to pollute the measured data flow time with the latency of the memory evaluation tool, the memory measuring runs were ran independently of the time measuring runs. 

    The memory sampling also takes up memory and because our test system has enough memory available, we bumped the maximum heap size up to 100GB, effectively eliminating memory timeouts.

    For this evaluation, we chose to use a non-default configuration of FlowDroid. 

    First, we disabled static field tracking due to the global scope as described in s:complexity. 

    Next, instead of the EasyTaintWrapper, we use the SummaryTaintWrapper, which utilizes StubDroid.

    We set the timeout for the data flow analysis to 10 minutes.

    A timeout in FlowDroid prevents processing new edges but lets the solver finish the current edge propagation. Thus, some apps may have a data flow time of above 600 seconds.

    The call graph generation was limited to 180 seconds and the call-graphs were serialized before, so every run was on the same call-graph. 

    The configuration summary is in t:realworldconfig.

    

                

        Real World Apps Configuration

                

    We did not use the full sources and sinks list included in FlowDroid because such would result in hundreds of sources and sinks per app and probably a long runtime.

    Instead, we chose to analyze which sensitive and possibly user-identifying data is sent out to the internet. 

    As we want to compare the forwards and backward implementation, it is also essential to not put one at a disadvantage. 

    We opted for a 2:1 ratio of sources to sinks. 

    This decision is based on the results of SuSi, a tool to automatically find sources and sinks in the Android framework. 

    Their extracted list of sources and sinks contains roughly  times more sources than sinks.

    The list of sources and sinks used in this evaluation is in t:realworldsources and t:realworldsinks.

    

                

        Sinks for Real World Apps Evaluation

            

    

                

        Sources for Real World Apps Evaluation

            

    We used FlowDroid's forward implementation on the to that date latest upstream commit(The latest upstream commit was at that time b436733fc4a5130dfe4ce8ddb3f76fd374e9a487.) from the develop branch for the point of comparison.

    The backward implementation ran on our latest commit with all changes merged from the upstream.

    We chose 200 apps randomly out of a Google Playstore dump from 2021 containing over 6000 apps for our evaluation set.

    Out of 200 apps, 60 apps do not have any sources or sinks and thus, the analysis did not start.

    For 6 apps, the analysis aborted with errors on at least one run. All thrown exceptions happened outside of FlowDroid.

    We are left with 131 apps for which both implementations completed all runs without errors.

    The full list is appended to this work in ?.

        Time Evaluation

    In general, the individual apps' runtime were far apart from each other. 

    We had many apps with a single-digit analysis time and on the other side, we also found many apps that triggered a timeout or were close to. 

    In between those extrema are only few apps.

    It is important to keep this in mind when interpreting the results.

    We first begin with an overview of the results. 

    t:realworldresults shows the results including timeouts. 

    Notably, the backward analysis had  less time timeouts than the forward analysis. 

    In return, it seems to be more memory hungry with  more memory timeouts. 

    We cover the memory consumption in the [s:memex]next subsection and place our focus on the time for now. 

    Interestingly, the propagated edges along the same interprocedural call-graph are of the same order of magnitude.

    Also, the 85th percentile runtime is nearly equal and the median is equal.  

    However, claims based on the runtime and edges with timeouts are only possible to a limited extent because both values are highly influenced by the timeout.

    

                

        Results With Timeouts

            

    Next, we only consider the runs without any timeouts in t:realworldresultswithouttimeout. 

    This time we can still observe a relation between backward infoflow edges and forward alias edges even though to a lesser extent. 

    More significant, backward needed way less forward propagations either less aliases were on the path or the alias analysis could be stopped earlier due to a near turn unit.

    The runtimes also represent this fact. In the 85th percentile, both analysis are more close than the average suggest with the backward analysis needing  seconds less. 

    The median here renders useless as a comparison point because of the huge variance in the data set.

    

                

        Results without Timeouts

            

    Now, lets look at it in greater detail. We now compare the analysis on a per-app basis. 

    The histogram is in f:deltaHist. 

    We compiled the delta data flow time of the analyses per app, calculated as in the last section with the forward implementation being the reference: . 

    Hence, negative values represent that our implementation performed better. 

    The delta on the x-axis is given is seconds and the frequency on the y-axis in number of apps. 

    The bins always span over  seconds.

    The graph shows a large number of apps around  with a slight bias towards the forward implementation. 

    Equivalent to the distribution of the data flow times, there are only few deltas in the range from  to . 

    More interestingly, there significant more apps around  than around . 

    Recall, the timeout is set to .

    So, our implementation terminates nearly instantanios in some cases on which the forward analysis times out.

    Concluding, as expected, there is no general advantage for a direction. 

    Instead, we only observe a per-app advantage in around  of the test set while for the rest, the performance is similar.

    In the direction-advantageous apps, we surprisingly observed a non-negligible amount of apps at the maximum possible delta.  

    

                

            

        

        Histogram of the Delta Data Flow Time

            

    To take advantage of the direction choice, we now investigate the correlating conditions for the advantageous direction. 

    Most straightforward would be a correlation between the difference of source and sink count and the data flow time. 

    In f:dfratio are two graphs with the ratio of sources and sinks () on the x-axis and the data flow time in seconds on the y-axis.

    The left graph is always the forward implementation and the right graph is our implementation. 

    Blue dots represent apps without timeout, orange a time timeout and red a memory timeout. 

    Intuitively, a negative ratio should put our implementation in a disadvantage. The graphs show no correlation between the ratio and the runtime, neither forward nor backward. 

    We also included the data flow time by sources and sinks in f:dfsources and f:dfsinks. 

    Forward, the number of sinks and backward the number of sources should not influence the runtime and as expected, they do not. 

    We can confirm Arzt's evaluation as there is no correlation between sources and the forward runtime in our app set. 

    Also, infered from this observation, we justifiably did not expect a correlation between sinks and backward runtime either. 

    Even though Arzt evaluation also showed no correlation between the code size, we do for completeness also compare the runtime to the number of statements, methods and classes. 

    Note that these refer to the Jimple intermediate representation and not Java. 

    f:dftocodesize includes all the graphs. 

    The arragement and notation is the same as before with the x-axis swapped out. 

    All graphs look similar with the majority of dots are in the left half of the graph.

    As per initial observation, the dots can be divided in two groups:

    those that are close to the ten minute mark and those which terminate nearly instantly. 

    The distribution of the dots inside the groups is approximately the same.

    Again, we can not observe a correlation.

    At last, we compare the number of edges in the exploded supergraph, also referred to taint propagations in s:complexity. 

    The graphs in f:dfedges show the edge count in comparison to the runtime. 

    In both graphs, the correlation between taint propagations and runtime is visible, especially in the right graph between  and . 

    The timeouts start in the backward graph after roughly  propagations.

    Forward, the threshold for timeouts is around  edge propagations, ealier than backward.

    To conclude, our backward analysis is efficient enough to be an alternative to the existing implementation. 

    We even found that it performed slightly better on our app set. 

    Our evaluation confirms that there is no correlation between an a-prioi known parameter and the runtime of FlowDroid - even in the backward direction. 

    Furthermore, we did not find any a-prioi known parameter to decide the favorable direction either. 

    The only parameter which correlates with the runtime is the edges in the exploded supergraph. 

    Now, these have shown that our implementation is capable of analysing roughly  more edges than the existing implementation in ten minutes. Though, the sample size of 200 apps is too small to generalize statements.

    

        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                            Data Flow Time in Comparison to Edge Count

            

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sources

                                    [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sinks

                                    [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Ratio

                            Data Flow Time in Comparison to Sources, Sinks and the Ratio of Those

            

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Statements

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Methods

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Classes

                Data Flow Time in Comparison to Code Size

            

        Memory Evaluation

    

                

        Memory Results

            

    Note that we only measured the memory usage of the edges in the exploded supergraph and not of the full program. 

    t:memres shows an overview of the results from the memory evaluation.  

    The measurements show an advantage for our implementation in all values. In total, the backward analysis needs in average about  less memory in the 85th percentile. Without timeouts the difference grows to over .

    

    In the time evaluation, we observed that our implementation has more memory timeouts but the result overview showed the backward analysis needs less memory in average.

    A manual look at those runs which had a memory timeout only backward revealed a larger exploded supergraph. 

    We argue that the memory timeouts are connected with the  more propagations in ten minutes and if the forward analysis had more time to run it would also trigger the memory timeout.

       

    

                

            

        

        Histogram of the Delta Maximum Memory Consumption

            

    Next, we look at the memory consumption difference per app in f:memHist. 

    The x-axis shows the delta maximum memory consumption in  and the y-axis the frequency. 

    Each bin is  wide.

    The delta is caluculated with forward as the refrence: . 

    Again, we see a gathering around . Otherwise, the histogram has a more uniform distribution than its [f:deltaHist]time counterpart.

    Still, there is a slight bias towards the backward analysis. 

    Because we only measured the exploded supergraph, there is a linear correlation between edges and memory usage (c.f. f:maxmemedges). 

    Similar, we observed a correlation between time and edges. Thus, this bias could be related to the faster backward analysis on the app set. We looked at this by comparing the sign of the delta data flow time with the sign of the delta memory consumption. 48 apps had different signs, with 23 being negligibily close to 0. Hence, the claim is true for 109 of 134 apps.

    Also benefical for the real-world usage of FlowDroid would be to be able to estimate the memory consumption to efficiently utilize the available resources. 

    In f:maxmemtoss, we contrast the memory consumption with the number of sources, sinks and the ratio of both. f:maxmemtocodesize shows the memory consumption in contrast to the statement, method and class count. 

    The arrangement and legend is the same as in the time evaluation.

    Unlike in the time evaluation, there is only one cluster of dots: those terminating nearly instantanios. Otherwise, the dots seem to be randomly distributed. All graphs indicate no correlation.

    To conclude, our backward analysis performed a bit better in the time evaluation which is also reflected in the memory consumption. Again, the results show that the observed edges are way more important for the memory consumption than the code size or the sources and sinks. It is not possible to estimate the memory consumption a-priori nor which analysis direction will use less memory. 

    

        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                            Maximum Memory Consumption in comparison to the Edge Count

            

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sources

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sinks

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Ratio

                Maximum Memory Consumption in Comparison to Source, Sink and Edge Count

            

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Statements

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Methods

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Classes

                Maximum Memory Consumption in Comparison to Code Size

                

    

    
