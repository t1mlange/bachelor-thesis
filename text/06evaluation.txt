    Performance Evaluation

    In the last chapter, we have shown that our implementation has the necessary soundness to be viable and yields the expected results.

    We now evaluate our implementation against the existing implementation in FlowDroid.

    DroidBench

    We already introduced Droidbench in s:droidbenchvalidation to validate the soundness of our backward-directed implementation.

    In this section, we focus on the performance in comparison to the existing forward-directed implementation in FlowDroid.

    DroidBench has the advantage that all apps are crafted explicitly for benchmarking taint analysis.

    So, most tests only contain a single-figure number of sources and sinks.

    Also, the number of sources and sinks are often equal or differ by one to test whether the tool can differentiate something.

    These simplify the comparison between both analysis directions as neither one has an initial disadvantage.

    Most test cases are small enough to be analyzed in sub-two seconds on an average four-core desktop CPU from 2012.

    Our test environment is not isolated, so background tasks and the process scheduler can affect the runtime.

    The short runtime, together with the variance of the unisolated testing environment, render the runtime unusable as a comparison point.

    In contrast, edge propagations are deterministic and correlate with the runtime.

    This is only true if there are enough resources. FlowDroid tries to gracefully terminate when running low on memory.

    Also, timeouts result in a non-reproducible number of edge propagations.

    Thus, we only use the number of propagations to compare both implementations.

    The configuration is the same as described in s:droidbenchconfig.

    Results

    The full results are listed in t:droidbenchevaluation.

    When rows only contain hyphens, the IFDS analysis did not start, e.g., because no sink is in the reachable code.

    I denotes the number of edge propagations inside the infoflow analysis and A the number of edge propagations inside the alias analysis.

    We calculated the absolute difference with the existing implementation as the reference: .

    The relative difference is calculated similar: .

    Hence, negative values signify the backward analysis performed better.

    On average, our implementation needs more edge propagations to finish the analysis.

    Even though for explicit flows the backward analysis needs less propagations in the infoflow analysis, it then suffers from more encountered aliases.

    If we look at it on a per test basis, there are not many test cases where both perform identically.

    Instead, dependent on the specific test case, the relative difference is between  and .

    However, we did not expect cases that let the edge propagations of our implementation explode up to a factor of , as seen in LifecycleTest#BroadcastReceiverLifecycle3.

    In contrast, the existing forward implementation only at most a relative difference of .

        

    

    Result Explanation

    We define tests with a relative difference greater than  as worth investigating.

    In the following, we explain why our implementation performed worse than expected.

    PrivateDataLeak3

    This test contains two sinks and one source.

    The tainted data is written to a file, later read from the file and then leaked.

    FlowDroid does not support tracking taints over files, so it only finds a leak from source to file write but misses the leak from file read to send SMS.

    Due to EasyTaintWrapper's simplicity, overtainting happens in the backward direction. When FileInputStream fis = openFileInput("out.txt"); is called with fis tainted, EasyTaintWrapper also taints the base object - the MainActivity in this case.

    As the MainActivity has an enormous scope, the taint has a long lifetime and many other taints could derive from this taint.

    This taint explains the relative difference of .

    Using the more precise SummaryTaintWrapper, the edges reduce to  and a relative difference of , which is more reasonable.

    It is still higher because of the second sink.

    MultiHandlers1

    Two LocationListeners are registered in different activities.

    In both activities, an instance field is a parameter of a sink.

    So there are two possible paths where something could be leaked.

    The LocationListener does not call any source on the first path, while the second path has an empty setter method killing the taint.

    For the first path, the backward analysis has to propagate the taint into the LocationListener to notice that this is a dead-end while the forward's search does not even start there.

    For the second path, the backward analysis seems to suffer because it starts at an instance field taint with a larger scope than a local variable.

    BroadcastReceiverLifecycle3

    The test contains five sinks but only one source.

    If we only consider the leak path, both implementations perform equally.

    The four other sinks are responsible for the overhead on edge propagations.

    Reflection6

    The reflective call site has multiple possible callees in the interprocedural control-flow graph.

    Backward all of these callees are visited, of which only one contains a source statement.

    Forward, the taint is introduced in the callee at the source and just one return site needs to be processed.

    A Note On Implicit Flows

    All implicit flow tests and the IMEI1 test need the implicit flow rule to find the leaks.

    In those test cases our implementation does not stand a chance.

    We especially want to highlight the "every sink call influenced by conditional" semantics here.

    This semantic forces us to derive an empty taint for every conditional that is theoretically reachable from a sink.

    Beyond, we also taint the base object without any fields at every sink to detect a possible conditional object instantiation.

    Even in simple test cases such as ImplicitFlow4 this results in 10 additional taints per sink.

    Important to note is also that the prior computation of reachable conditionals is not represented in the edge propagations.

    We thus conclude that it is probably better to live without a backward-directed implicit data flow analysis.

    Using A More Precise Taint Wrapper

    We noticed the overtainting in PrivateDataLeak3 is caused by the EasyTaintWrapper.

    Thus, we now look how using the SummaryTaintWrapper influences the edge propagations.

    The full results are in t:droidbenchevaluation_sum.

    In the table, we take the EasyTaintWrapper as the reference and compare it against the SummaryTaintWrapper on our implementation.

    The structure of the table is as in the last subsection.

    As we already described, PrivateDataLeak3 benefits from the more precise taint wrapper.

    Similarily, many other test cases also benefit.

    Others, especially Serialization1 have more edge propagations because the SummaryTaintWrapper has a summary for a method which the EasyTaintWrapper does not handle resulting in a premature kill of a taint.

    The EasyTaintWrapper contains a list of supported classes. Every method from those classes is excluded from the analysis, regardless of the method being in the list of the handled methods.

    Even with Serialization1 included in the average, the SummaryTaintWrapper needs less total edge propagations.

    Excluding it also equals out the relative difference.

    Altogether, the SummaryTaintWrapper should be the default choice for real-world applications because it is more precise without compromising on the edge propagations.

        

    

    Real World Apps

    Configuration

    Our test machine is equipped with four Intel Xeon E5-4650 and 1 TB of RAM.

    We limited the JVM to 50 GB RAM and FlowDroid on 16 threads per instance.

    We ran at most four instances in parallel to ensure a one-to-one mapping between CPU threads and FlowDroid threads.

    Note that the test machine is a shared system, but we made sure there are always enough resources for our evaluation available.

    Still, background services might influence the performance of a single run. To stamp out this factor, we ran each app three times with a distance of time.

    The time distance between each run is at least the elapsed time from the analysis of the remaining 199 apps.

    If there were outliers, we repeated the run.

    (Outliers are runs with at least  difference to the median run and a minimum of  seconds absolute difference.)

    Some runs did not comply to our outlier norm even after we ran them multiple times, but this only concers 9 out of 600 runs.

    We also measured the memory usage of both implementations.

    Using the memory amount reported by the JVM is not precise because the JVM prefers to take up free memory before running the garbage collector.

    We borrowed the memory evaluation tool from CleanDroid, which internally depends on a memory calculation tool from Twitter(https://mvnrepository.com/artifact/com.twitter.common/objectsize (visited on 18.04.2021)).

    The memory evaluation tool measures the size of the exploded supergraph in 15 seconds intervals.

    Because we do not want to pollute the measured data flow time with the memory evaluation tool's latency, the memory measuring runs were run independently of the time measuring runs.

    The memory sampling also takes up memory and because our test system has enough memory available, we bumped the maximum heap size up to 100GB, effectively eliminating memory timeouts.

    For this evaluation, we chose to use a non-default configuration of FlowDroid.

    First, we disabled static field tracking due to the global scope as described in s:complexity.

    Next, instead of the EasyTaintWrapper, we use the SummaryTaintWrapper, which utilizes StubDroid.

    We set the timeout for the data flow analysis to 10 minutes.

    A timeout in FlowDroid prevents processing new edges but lets the solver finish the current edge propagation. Thus, some apps may have a data flow time of above 600 seconds.

    The call graph generation was limited to 180 seconds and the call-graphs were serialized before, so every run was on the same call-graph.

    The configuration summary is in t:realworldconfig.

    

                

        Real World Apps Configuration

            

    We did not use the full sources and sinks list included in FlowDroid because such would result in hundreds of sources and sinks per app and probably a long runtime.

    Instead, we chose to analyze which sensitive and possibly user-identifying data is sent out to the internet.

    As we want to compare the forwards and backward implementation, it is also essential to not put one at a disadvantage.

    We opted for a 2:1 ratio of sources to sinks.

    This decision is based on the results of SuSi, to find sources and sinks in the Android framework automatically.

    Their extracted list of sources and sinks contains roughly  times more sources than sinks.

    The list of sources and sinks used in this evaluation is in t:realworldsources and t:realworldsinks.

    

                

        Sources for Real World Apps Evaluation

            

    

                

        Sinks for Real World Apps Evaluation

            

    We used FlowDroid's forward implementation on the to that date latest upstream commit(The latest upstream commit was at that time b436733fc4a5130dfe4ce8ddb3f76fd374e9a487.) from the develop branch for the point of comparison.

    The backward implementation ran on our latest commit at that time with all changes from the upstream merged into.

    Our latest commit was 87bf33ba40ef8b4fb25f33439d887ebc98c2c184. Note that during the real-world evaluation we found some bugs and also later on fixed some edge cases in the analysis. All fixes should not influence the runtime in a bad way.

    We chose 200 apps randomly out of a Google Playstore dump from 2021 containing over 6000 apps for our evaluation set.

    Out of 200 apps, 60 apps do not have any sources or sinks and thus, the analysis did not start.

    For six apps, the analysis aborted with errors on at least one run. All thrown exceptions happened outside of FlowDroid.

    We are left with 131 apps for which both implementations completed all runs without errors.

    The full list is appended to this work in a:appset.

        Time Evaluation

    In general, the individual apps' runtimes were far apart from each other.

    We had many apps with a single-digit analysis time and on the other side, we also found many apps that triggered a timeout or were close to triggering one.

    In between those extrema are only a few apps.

    Recall that we set a soft limit on the runtime at 600 seconds.

    The reference forward runs have a standard deviation of  seconds and the runs of our implementation has  seconds standard deviation.

    It is important to keep this in mind when interpreting the results.

    We first begin with an overview of the results.

    t:realworldresults shows the results, including timeouts.

    Notably, the backward analysis had  less time timeouts than the forward analysis.

    In return, it seems a bit more memory-hungry with  more memory timeouts.

    We conducted a t-test to check the significance of those differences with the null hypothesis of equal average expected values.

    The p-value for the memory timeouts is , thus being insignificant.

    A t-test over the runtime yielded a p-value of , meaning the advantage for our implementation is significant.

    We cover the memory consumption extensively in the [s:memex]next subsection and focus on the time for now.

    Interestingly, the propagated edges along the same interprocedural call-graph are of the same order of magnitude.

    Also, the 85th percentile runtime is nearly equal and the median is equal.

    However, claims based on the runtime and edges with timeouts are only possible to a limited extent because the timeout highly influences both values.

    

                

        Results With Timeouts

            

    Next, we only consider the runs without any timeouts in t:realworldresultswithouttimeout.

    This time we the relation between backward infoflow edges and forward alias edges is present to a lesser extent, maybe even non-existent.

    More significant, backward needed less propagations in average and in the 85th percentile.

    This fact is also represented in the runtimes. 

    The average runtime is the half of the existing implementation.

    Though because of the diverse data, the average is not convincing.

    The median is equally at zero for both.

    This is partly based on our measuring where we only measure the time in seconds and round it down.

    Thus, all runs with a runtime of zero seconds did terminate in less than 1 second.

    In the 85th percentile, more significant given our data set, the backward analysis needs  seconds less.

    

                

        Results without Timeouts

            

    A knowledgeable reader might have noticed the results in t:realworldresults are worse than in previous publications where FlowDroid was evaluated.

    We want to emphasize that none of our changes did influence the reference runs in a bad way as we used the upstream version without a single line changed to conduct the first run.

    We found some exceptions in the upstream project while evaluating, so after the first run we switched to our version with the fixes included. This did not change the results we got.

    The existing implementation suffers as ours, so we suspect it partly depends on an unfortunatly drawn app set and further development in the call-graph generation leading to more possible edges.

    With that out of the way, let us look at the results in greater detail. We now compare the analysis on a per-app basis.

    The histogram is in f:deltaHist.

    We compiled the delta data flow time of the analyses per app, calculated as in the last section with the forward implementation being the reference: .

    Hence, negative values represent that our implementation performed better.

    The delta on the x-axis is given in seconds and the frequency on the y-axis in number of apps.

    The bins always span over  seconds.

    The graph shows a large number of apps around  with a slight bias towards the forward implementation.

    Equivalent to the distribution of the data flow times, there are only few deltas in the range from  to .

    More interestingly, there are significantly more apps around  than around .

    Recall, the timeout is set to .

    So, our implementation terminates nearly instantaneous in some cases on which the forward analysis times out.

    As expected, there is no general advantage for a direction.

    Instead, we observe a per-app advantage in around  of the test set, while for the rest, the performance is similar.

    

                

            

        

        Histogram of the Delta Data Flow Time

            

    We confirmed that the right direction choice can speed up the analysis by a magnificent amount. To take advantage of the favorable direction, we now investigate the correlating conditions for the advantageous direction.

    Most straightforward would be a correlation between the difference of source and sink count and the data flow time.

    In f:dfratio are two graphs with the ratio of sources and sinks () on the x-axis and the data flow time in seconds on the y-axis.

    The left graph is always the forward implementation and the right graph is our implementation.

    Blue dots represent apps without a timeout, orange a time timeout and red a memory timeout.

    Intuitively, a negative ratio should put our implementation at an advantage. The graphs show no correlation between the ratio and the runtime, neither forward nor backward.

    We also included the forward data flow time by sources and the backward data flow time by sinks in f:dfsources and f:dfsinks.

    The number of sinks backward and the number of sources forward do not influence the runtime.

    So we can confirm Arzt's evaluation as there is no correlation between sources and the forward runtime in our app set.

    Parallel to this observation, the sink count does not influence the backward runtime.

    The sink count for forward and the source count for backward can not influence the runtime they have no influence on the edge propagations.

    

                [b]0.45

                        []

                                

                    

                

                        Source Count

                                    [b]0.45

                        []

                                

                    

                

                        Sink Count

                                    [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Ratio

                            Data Flow Time in Comparison to Sources, Sinks and the Ratio of Those

            

    Even though Arzt's evaluation also showed no correlation between the code size, we do for completeness also compare the runtime to the number of statements, methods and classes.

    Note that these refer to the Jimple intermediate representation and not Java.

    f:dftocodesize includes all graphs with the existing implementation being on the left side and our implementation on the right side.

    The notation are the same as before, with the x-axis swapped out.

    The number of statements is uniformly distributed with some outliers.

    If we consider our data as two linear data sets with a structural break between the two groups, the linear regressions have a slope of close to 0.

    Resulting, the number of statements, methods and classes do not have an impact on the runtime.

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Statements

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Methods

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Classes

                Data Flow Time in Comparison to Code Size

            

    Because the above mentioned parameters do not influence the runtime, we did further investigate to find a parameter to decide the favorable direction.

    First, we looked the methods containing sources and sinks.

    We counted the number of statements of the method, call statements and callers of the method and compared these numbers between sources and sinks.

    A advantage in those did not result in a faster analysis.

    Next, we implemented a fast intraprocedural taint analysis. It omits access paths and aliasing.

    Method calls are overapproximated in a similar fashion to the EasyTaintWrapper.

    We then counted the taints flowing into the callees and callers.

    Also, we did count the number of taints in the method.

    The drawback is that this only works when the state explosion happens inside the first method and this is not the case in the app set.

    Again, we could not find any resilient correlation.

    At this point, we run out of easily computable facts about an app that could correlate with the runtime and decided to leave the question up for future work.

    Finally, we compare the number of edges in the exploded supergraph, referred to as taint propagations in s:complexity.

    Note that the edges in the exploded supergraph are only known after the analysis, making them useless for predictions.

    In f:dfedgestotal we plot the edge count on the x-axis to the data flow time on the y-axis.

    In both graphs, we observe a linear correlation for the apps with a runtime below 500 seconds.

    Then there is a structural break and after that the apps time out.

    Because a linear regression does not really fit well for our diverse data set, we decided to fit a function using the least squares method. We achieved a  measure of greater than  for four degree polynomials and above.

    However, the good fitting curve seems overfitted to us because the apps not being close to timeouts have a good fitting linear correlation.

    When we look at the point where the structural break happens, we notice that backward the timeouts start after roughly  propagations.

    Forward on the other hand only gets to around  edge propagations before reaching a timeout.

    Such a large difference is unintuitive because the computation cost should not be much different.

    We split the edges up by IFDS problems in f:dfedgesi and f:dfedgesa.

    Interestingly, all curves have a similar steep curve with the exception of the backward alias analysis being more shallow.

    This gives a possible explanation linked to the ratio of infoflow and alias edges.

    The alias flow functions are way simpler and thus, should also cost less to compute.

    The backward analysis has a ratio biased toward the alias edges which could explain the higher edge count possible in ten minutes.

    Why the structural break happens could not be conclusively clarified in this work, so it is also hard to finally reason whether this also holds without such a structural break.

    

        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Total Edges

                                    [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Infoflow Edges

                                    [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Alias Edges

                            Data Flow Time in Comparison to Edge Count

            

    To conclude, our backward analysis is efficient enough to be an alternative to the existing implementation.

    We even found that it performed slightly better on our app set.

    Our evaluation shows that there is no correlation between an apriori known parameter and the runtime of FlowDroid - in both directions.

    Furthermore, we did not find any apriori known parameter to decide the favorable direction either.

    The edge propagations have shown that our implementation can analyze roughly  more edges than the existing implementation in ten minutes. Though, the sample size of 200 apps is too small to generalize statements and our data was rather challenging to interpret with a large standard deviation.

        Memory Evaluation

    

                

        Memory Results

            

    t:memres shows an overview of the results from the memory evaluation.

    Note that we only measured the memory usage of the edges in the exploded supergraph and not of the full program.

    Also, unlike the time measurements, the memory consumption is much more distributed distributed across the range.

    The measurements with timeouts show similar values for both directions with a bias toward our implementation.

    Though, maximum measurements with timeouts are not really meaningful because of the cut-off at ten minutes.

    Without timeouts the gap gets a bit bigger.

    The average maximum memory consumption of our implementation is around  lower than the existing one. 

    Especially in the 85th percentile our implementation shines where it needs  less memory.

    Next, we look at the memory consumption difference per app in f:memHist.

    The x-axis shows the delta maximum memory consumption in megabytes and the y-axis the frequency.

    Each bin is  wide.

    The delta is calculated with forward as the refrence: .

    Again, we see a gathering around . 

    Otherwise, the histogram has a more uniform distribution than its [f:deltaHist]time counterpart.

    Just as in the overview, there is a slight bias towards the backward analysis.

    We argue this bias is related to the faster backward analysis in the app set. 

    A faster analysis means less edges and the edges should correlate linearly with the memory usage of the exploded supergraph.

    This was at least true for apps without timeout. 

    We looked at this by comparing the sign of the delta data flow time with the sign of the delta memory consumption. 

    48 apps had different signs, with 23 being negligibly close to 0. 

    Hence, the claim is true for 109 of 134 apps.

    We also calculated the Pearson correlation coefficient for the data flow time and the maximum memory consumption.

    For the forward analysis the coefficient is  and  for the backward implementation.

    Both values indicate a correlation.

    

                

            

        

        Histogram of the Delta Maximum Memory Consumption

            

    For completeness, we now validate our claim that the edges in the exploded supergraph correlate linearly with the memory consumption.

    We expect a linear correlation because the exploded supergraph is represented as a HashMap of edges and taints. 

    Consider f:maxmemedges where a clear, linear correlation is visible.

    We confirmed our claim.

    

        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                            Maximum Memory Consumption in Comparison to the Edge Count

            

    Also beneficial for the real-world usage of FlowDroid would be to estimate the memory consumption to utilize the available resources efficiently.

    In f:maxmemtoss, we contrast the memory consumption with the number of sources, sinks and the ratio of both.

    f:maxmemtocodesize shows the memory consumption in contrast to the statement, method and class count.

    The arrangement and legend are the same as in the time evaluation.

    Unlike in the time evaluation, there is only one cluster of dots: those terminating nearly instantaneous. 

    Otherwise, the dots seem to be randomly distributed. 

    All graphs indicate no correlation.

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sources

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Sinks

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Ratio

                Maximum Memory Consumption In Comparison To Source, Sink And Edge Count

            

    

                [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Statements

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Methods

                        [b]

                        []0.45

                                

                    

                

                                    []0.45

                                

                    

                

                        Classes

                Maximum Memory Consumption in Comparison to Code Size

            

    To conclude, our backward analysis performed a bit better in the time evaluation, which is also reflected in the memory consumption. Again, the results show that the observed edges are way more important for memory consumption than the code size or the sources and sinks. It is not possible to estimate the memory consumption prior nor which analysis direction will use less memory.

    

    
