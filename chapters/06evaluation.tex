\documentclass[../draft.tex]{subfiles}

\newcommand{\tsubEight}[1]{\multicolumn{9}{c}{#1}\\\hline}

\begin{document}
    \chapter{Performance Evaluation}
    In the last chapter, we have shown that our implementation has the necessary soundness to be viable and yields the expected results.
    We now evaluate our implementation against the existing implementation in \textsc{FlowDroid}.

    \section{DroidBench}
    We already introduced \textsc{Droidbench} in \autoref{s:droidbenchvalidation} to validate the soundness of our backward-directed implementation.
    In this section, we focus on the performance in comparison to the existing forward-directed implementation in \textsc{FlowDroid}.

    DroidBench has the advantage that all apps are crafted explicitly for benchmarking taint analysis.
    So, most tests only contain a single-figure number of sources and sinks.
    Also, the number of sources and sinks are often equal or differ by one to test whether the tool can differentiate something.
    These simplify the comparison between both analysis directions as neither one has an initial disadvantage.

    Most test cases are small enough to be analyzed in sub-two seconds on an average four-core desktop CPU from 2012.
    Our test environment is not isolated, so background tasks and the process scheduler can affect the runtime.
    The short runtime, together with the variance of the unisolated testing environment, render the runtime unusable as a comparison point.
    In contrast, edge propagations are deterministic\footnotemark{} and correlate with the runtime.
    \footnotetext{This is only true if there are enough resources. \textsc{FlowDroid} tries to gracefully terminate when running low on memory.
    Also, timeouts result in a non-reproducible number of edge propagations.}
    Thus, we only use the number of propagations to compare both implementations.

    The configuration is the same as described in \autoref{s:droidbenchconfig}.

    \subsection{Results}
    The full results are listed in \autoref{t:droidbenchevaluation}.
    When rows only contain hyphens, the IFDS analysis did not start, e.g., because no sink is in the reachable code.
    \#I denotes the number of edge propagations inside the infoflow analysis and \#A the number of edge propagations inside the alias analysis.
    We calculated the absolute difference with the existing implementation as the reference: $\mathit{Result}_{\mathit{B}} - \mathit{Result}_{\mathit{F}}$.
    The relative difference is calculated similar: $\frac{\mathit{TotalDifference}}{|\mathit{\#I_\mathit{F} + \#A_\mathit{F}}|}$.
    Hence, negative values signify the backward analysis performed better.

    On average, our implementation needs more edge propagations to finish the analysis.
    Even though for explicit flows the backward analysis needs less propagations in the infoflow analysis, it then suffers from more encountered aliases.
    If we look at it on a per test basis, there are not many test cases where both perform identically.
    Instead, dependent on the specific test case, the relative difference is between $-1$ and $1$.
    However, we did not expect cases that let the edge propagations of our implementation explode up to a factor of $100$, as seen in \code{LifecycleTest#BroadcastReceiverLifecycle3}.
    In contrast, the existing forward implementation only at most a relative difference of $-0.95$.

    \footnotesize
    \begin{longtable}{l | r | r | r | r | r | r | r | r}
        & \multicolumn{2}{c|}{\textbf{Forwards}} & \multicolumn{2}{c|}{\textbf{Backwards}} & \multicolumn{4}{c}{\textbf{Difference}}\\
        \multirow{-2}{*}{\textbf{Test Case}} & \textbf{\#I} & \textbf{\#A} & \textbf{\#I} & \textbf{\#A} & \textbf{\#I} & \textbf{\#A}& \textbf{Total} & \textbf{Relative}\\
        \hhline
        \endhead
        \tsubEight{AliasingTest}
        FlowSensitivity1 & $175$ & $72$ & $39$ & $4$ & $-136$ & $-68$ & $-204$ & $-0.83$\\
        Merge1 & $137$ & $65$ & $89$ & $47$ & $-48$ & $-18$ & $-66$ & $-0.33$\\
        SimpleAliasing1 & $35$ & $13$ & $20$ & $3$ & $-15$ & $-10$ & $-25$ & $-0.52$\\
        StrongUpdate1 & $30$ & $13$ & $11$ & $3$ & $-19$ & $-10$ & $-29$ & $-0.67$\\
        \hline
        \tsubEight{AndroidSpecificTest}
        ApplicationModeling1 & $212$ & $96$ & $851$ & $1208$ & $639$ & $1112$ & $1751$ & $5.69$\\
        DirectLeak1 & $3$ & $0$ & $4$ & $0$ & $1$ & $0$ & $1$ & $0.33$\\
        InactiveActivity & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        Library2 & $5$ & $0$ & $6$ & $0$ & $1$ & $0$ & $1$ & $0.2$\\
        LogNoLeak & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        Obfuscation1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Parcel1 & $144$ & $15$ & $86$ & $93$ & $-58$ & $78$ & $20$ & $0.13$\\
        PrivateDataLeak1 & $410$ & $110$ & $608$ & $766$ & $198$ & $656$ & $854$ & $1.64$\\
        PrivateDataLeak2 & $15$ & $0$ & $5$ & $3$ & $-10$ & $3$ & $-7$ & $-0.47$\\
        PrivateDataLeak3 & $17$ & $2$ & $210$ & $140$ & $193$ & $138$ & $331$ & $17.42$\\
        runPublicAPIField1 & $89$ & $1$ & $43$ & $0$ & $-46$ & $-1$ & $-47$ & $-0.52$\\
        runPublicAPIField2 & $5$ & $0$ & $11$ & $0$ & $6$ & $0$ & $6$ & $1.2$\\
        runView1 & $71$ & $50$ & $69$ & $0$ & $-2$ & $-50$ & $-52$ & $-0.43$\\
        \hline
        \tsubEight{ArrayAndListTest}
        ArrayAccess1 & $77$ & $34$ & $51$ & $100$ & $-26$ & $66$ & $40$ & $0.36$\\
        ArrayAccess2 & $16$ & $4$ & $12$ & $0$ & $-4$ & $-4$ & $-8$ & $-0.4$\\
        ArrayAccess3 & $77$ & $34$ & $51$ & $100$ & $-26$ & $66$ & $40$ & $0.36$\\
        ArrayAccess4 & $164$ & $84$ & $42$ & $21$ & $-122$ & $-63$ & $-185$ & $-0.75$\\
        ArrayAccess5 & $75$ & $5$ & $34$ & $23$ & $-41$ & $18$ & $-23$ & $-0.29$\\
        ArrayCopy1 & $18$ & $2$ & $9$ & $2$ & $-9$ & $0$ & $-9$ & $-0.45$\\
        ArrayToString1 & $10$ & $1$ & $6$ & $0$ & $-4$ & $-1$ & $-5$ & $-0.45$\\
        HashMapAccess1 & $22$ & $5$ & $15$ & $1$ & $-7$ & $-4$ & $-11$ & $-0.41$\\
        ListAccess1 & $85$ & $9$ & $60$ & $97$ & $-25$ & $88$ & $63$ & $0.67$\\
        MultidimensionalArray1 & $29$ & $3$ & $16$ & $23$ & $-13$ & $20$ & $7$ & $0.22$\\
        \hline
        \tsubEight{CallbackTest}
        AnonymousClass1 & $152$ & $0$ & $208$ & $0$ & $56$ & $0$ & $56$ & $0.37$\\
        Button1 & $58$ & $39$ & $43$ & $0$ & $-15$ & $-39$ & $-54$ & $-0.56$\\
        Button2 & $444$ & $66$ & $155$ & $254$ & $-289$ & $188$ & $-101$ & $-0.2$\\
        Button3 & $360$ & $89$ & $109$ & $408$ & $-251$ & $319$ & $68$ & $0.15$\\
        Button4 & $58$ & $39$ & $43$ & $0$ & $-15$ & $-39$ & $-54$ & $-0.56$\\
        Button5 & $80$ & $40$ & $6$ & $3$ & $-74$ & $-37$ & $-111$ & $-0.93$\\
        LocationLeak1 & $617$ & $222$ & $260$ & $298$ & $-357$ & $76$ & $-281$ & $-0.33$\\
        LocationLeak2 & $212$ & $121$ & $152$ & $0$ & $-60$ & $-121$ & $-181$ & $-0.54$\\
        LocationLeak3 & $220$ & $73$ & $104$ & $115$ & $-116$ & $42$ & $-74$ & $-0.25$\\
        MethodOverride1 & $3$ & $0$ & $2$ & $0$ & $-1$ & $0$ & $-1$ & $-0.33$\\
        MultiHandlers1 & $17$ & $0$ & $145$ & $149$ & $128$ & $149$ & $277$ & $16.29$\\
        Ordering1 & $456$ & $151$ & $44$ & $0$ & $-412$ & $-151$ & $-563$ & $-0.93$\\
        RegisterGlobal1 & $291$ & $162$ & $49$ & $0$ & $-242$ & $-162$ & $-404$ & $-0.89$\\
        RegisterGlobal2 & $52$ & $37$ & $43$ & $0$ & $-9$ & $-37$ & $-46$ & $-0.52$\\
        Unregister1 & $11$ & $0$ & $9$ & $0$ & $-2$ & $0$ & $-2$ & $-0.18$\\
        \hline
        \tsubEight{EmulatorDetectionTest}
        Battery1 & $7$ & $0$ & $39$ & $15$ & $32$ & $15$ & $47$ & $6.71$\\
        Bluetooth1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Build1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Contacts1 & $53$ & $0$ & $200$ & $19$ & $147$ & $19$ & $166$ & $3.13$\\
        ContentProvider1 & $13$ & $0$ & $8$ & $0$ & $-5$ & $0$ & $-5$ & $-0.38$\\
        DeviceId1 & $15$ & $0$ & $6$ & $0$ & $-9$ & $0$ & $-9$ & $-0.6$\\
        File1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        IMEI1 & $137$ & $0$ & $422$ & $1$ & $285$ & $1$ & $286$ & $2.09$\\
        IP1 & $4$ & $0$ & $29$ & $0$ & $25$ & $0$ & $25$ & $6.25$\\
        PI1 & $6$ & $0$ & $4$ & $0$ & $-2$ & $0$ & $-2$ & $-0.33$\\
        PlayStore1 & $158$ & $0$ & $8$ & $0$ & $-150$ & $0$ & $-150$ & $-0.95$\\
        PlayStore2 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Sensors1 & $5$ & $0$ & $4$ & $0$ & $-1$ & $0$ & $-1$ & $-0.2$\\
        SubscriberId1 & $29$ & $0$ & $4$ & $0$ & $-25$ & $0$ & $-25$ & $-0.86$\\
        VoiceMail1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{FieldAndObjectSensitivityTest}
        FieldSensitivity1 & $98$ & $50$ & $25$ & $3$ & $-73$ & $-47$ & $-120$ & $-0.81$\\
        FieldSensitivity2 & $35$ & $15$ & $19$ & $0$ & $-16$ & $-15$ & $-31$ & $-0.62$\\
        FieldSensitivity3 & $38$ & $15$ & $16$ & $0$ & $-22$ & $-15$ & $-37$ & $-0.7$\\
        FieldSensitivity4 & $14$ & $6$ & $8$ & $0$ & $-6$ & $-6$ & $-12$ & $-0.6$\\
        InheritedObjects1 & $4$ & $0$ & $6$ & $0$ & $2$ & $0$ & $2$ & $0.5$\\
        ObjectSensitivity1 & $19$ & $7$ & $14$ & $1$ & $-5$ & $-6$ & $-11$ & $-0.42$\\
        ObjectSensitivity2 & $15$ & $8$ & $10$ & $0$ & $-5$ & $-8$ & $-13$ & $-0.57$\\
        \hline
        \tsubEight{GeneralJavaTest}
        Clone1 & $23$ & $2$ & $12$ & $4$ & $-11$ & $2$ & $-9$ & $-0.36$\\
        Exceptions1 & $16$ & $0$ & $13$ & $0$ & $-3$ & $0$ & $-3$ & $-0.19$\\
        Exceptions2 & $22$ & $0$ & $13$ & $0$ & $-9$ & $0$ & $-9$ & $-0.41$\\
        Exceptions3 & $18$ & $0$ & $11$ & $0$ & $-7$ & $0$ & $-7$ & $-0.39$\\
        Exceptions4 & $21$ & $1$ & $22$ & $0$ & $1$ & $-1$ & $0$ & $0.0$\\
        Exceptions5 & $13$ & $1$ & $16$ & $0$ & $3$ & $-1$ & $2$ & $0.14$\\
        Exceptions6 & $78$ & $12$ & $23$ & $0$ & $-55$ & $-12$ & $-67$ & $-0.74$\\
        Exceptions7 & $71$ & $12$ & $6$ & $0$ & $-65$ & $-12$ & $-77$ & $-0.93$\\
        FactoryMethods1 & $40$ & $0$ & $14$ & $0$ & $-26$ & $0$ & $-26$ & $-0.65$\\
        Loop1 & $93$ & $2$ & $51$ & $0$ & $-42$ & $-2$ & $-44$ & $-0.46$\\
        Loop2 & $123$ & $2$ & $79$ & $0$ & $-44$ & $-2$ & $-46$ & $-0.37$\\
        Serialization1 & $50$ & $4$ & $22$ & $29$ & $-28$ & $25$ & $-3$ & $-0.06$\\
        SourceCodeSpecific1 & $16$ & $0$ & $45$ & $7$ & $29$ & $7$ & $36$ & $2.25$\\
        StartProcessWithSecret1 & $29$ & $8$ & $17$ & $3$ & $-12$ & $-5$ & $-17$ & $-0.46$\\
        StaticInitialization1 & $26$ & $27$ & $9$ & $0$ & $-17$ & $-27$ & $-44$ & $-0.83$\\
        StaticInitialization2 & $57$ & $29$ & $86$ & $0$ & $29$ & $-29$ & $0$ & $0.0$\\
        StaticInitialization3 & $35$ & $9$ & $5$ & $0$ & $-30$ & $-9$ & $-39$ & $-0.89$\\
        StringFormatter1 & $16$ & $1$ & $10$ & $0$ & $-6$ & $-1$ & $-7$ & $-0.41$\\
        StringPatternMatching1 & $23$ & $1$ & $8$ & $4$ & $-15$ & $3$ & $-12$ & $-0.5$\\
        StringToCharArray1 & $91$ & $4$ & $47$ & $0$ & $-44$ & $-4$ & $-48$ & $-0.51$\\
        StringToOutputStream1 & $26$ & $3$ & $25$ & $1$ & $-1$ & $-2$ & $-3$ & $-0.1$\\
        UnreachableCode & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        VirtualDispatch1 & $128$ & $31$ & $88$ & $28$ & $-40$ & $-3$ & $-43$ & $-0.27$\\
        VirtualDispatch2 & $7$ & $0$ & $12$ & $0$ & $5$ & $0$ & $5$ & $0.71$\\
        VirtualDispatch3 & $8$ & $0$ & $6$ & $0$ & $-2$ & $0$ & $-2$ & $-0.25$\\
        VirtualDispatch4 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        \hline
        \tsubEight{ImplicitFlowTest}
        ImplicitFlow1 & $1823$ & $144$ & $3315$ & $11$ & $1492$ & $-133$ & $1359$ & $0.69$\\
        ImplicitFlow2 & $146$ & $63$ & $991$ & $3$ & $845$ & $-60$ & $785$ & $3.76$\\
        ImplicitFlow3 & $148$ & $50$ & $1023$ & $20$ & $875$ & $-30$ & $845$ & $4.27$\\
        ImplicitFlow4 & $67$ & $0$ & $1864$ & $12$ & $1797$ & $12$ & $1809$ & $27.0$\\
        ImplicitFlow6 & $18$ & $0$ & $112$ & $0$ & $94$ & $0$ & $94$ & $5.22$\\
        \hline
        \tsubEight{LifecycleTest}
        ActivityEventSequence1 & $58$ & $35$ & $72$ & $0$ & $14$ & $-35$ & $-21$ & $-0.23$\\
        ActivityEventSequence2 & $32$ & $24$ & $77$ & $0$ & $45$ & $-24$ & $21$ & $0.38$\\
        ActivityEventSequence3 & $209$ & $116$ & $156$ & $0$ & $-53$ & $-116$ & $-169$ & $-0.52$\\
        ActivityLifecycle1 & $99$ & $72$ & $156$ & $7$ & $57$ & $-65$ & $-8$ & $-0.05$\\
        ActivityLifecycle2 & $47$ & $34$ & $33$ & $0$ & $-14$ & $-34$ & $-48$ & $-0.59$\\
        ActivityLifecycle3 & $65$ & $31$ & $28$ & $0$ & $-37$ & $-31$ & $-68$ & $-0.71$\\
        ActivityLifecycle4 & $49$ & $33$ & $14$ & $0$ & $-35$ & $-33$ & $-68$ & $-0.83$\\
        ActivitySavedState1 & $20$ & $0$ & $7$ & $0$ & $-13$ & $0$ & $-13$ & $-0.65$\\
        ApplicationLifecycle1 & $37$ & $10$ & $82$ & $0$ & $45$ & $-10$ & $35$ & $0.74$\\
        ApplicationLifecycle2 & $86$ & $17$ & $94$ & $155$ & $8$ & $138$ & $146$ & $1.42$\\
        ApplicationLifecycle3 & $32$ & $12$ & $21$ & $0$ & $-11$ & $-12$ & $-23$ & $-0.52$\\
        AsynchronousEventOrdering1 & $58$ & $31$ & $16$ & $0$ & $-42$ & $-31$ & $-73$ & $-0.82$\\
        BroadcastReceiverLifecycle1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        BroadcastReceiverLifecycle2 & $109$ & $44$ & $248$ & $114$ & $139$ & $70$ & $209$ & $1.37$\\
        BroadcastReceiverLifecycle3 & $3$ & $0$ & $195$ & $110$ & $192$ & $110$ & $302$ & $100.67$\\
        EventOrdering1 & $61$ & $29$ & $30$ & $0$ & $-31$ & $-29$ & $-60$ & $-0.67$\\
        FragmentLifecycle1 & $187$ & $127$ & $90$ & $0$ & $-97$ & $-127$ & $-224$ & $-0.71$\\
        FragmentLifecycle2 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        ServiceEventSequence1 & $53$ & $20$ & $124$ & $34$ & $71$ & $14$ & $85$ & $1.16$\\
        ServiceEventSequence2 & $105$ & $49$ & $389$ & $220$ & $284$ & $171$ & $455$ & $2.95$\\
        ServiceEventSequence3 & $46$ & $12$ & $275$ & $151$ & $229$ & $139$ & $368$ & $6.34$\\
        ServiceLifecycle1 & $119$ & $44$ & $42$ & $0$ & $-77$ & $-44$ & $-121$ & $-0.74$\\
        ServiceLifecycle2 & $68$ & $20$ & $89$ & $21$ & $21$ & $1$ & $22$ & $0.25$\\
        SharedPreferenceChanged1 & $13$ & $0$ & $11$ & $0$ & $-2$ & $0$ & $-2$ & $-0.15$\\
        \hline
        \tsubEight{ReflectionTest}
        Reflection1 & $15$ & $5$ & $8$ & $0$ & $-7$ & $-5$ & $-12$ & $-0.6$\\
        Reflection2 & $21$ & $5$ & $11$ & $0$ & $-10$ & $-5$ & $-15$ & $-0.58$\\
        Reflection3 & $42$ & $9$ & $62$ & $25$ & $20$ & $16$ & $36$ & $0.71$\\
        Reflection4 & $9$ & $0$ & $8$ & $0$ & $-1$ & $0$ & $-1$ & $-0.11$\\
        Reflection5 & $16$ & $1$ & $11$ & $0$ & $-5$ & $-1$ & $-6$ & $-0.35$\\
        Reflection6 & $7$ & $0$ & $134$ & $51$ & $127$ & $51$ & $178$ & $25.43$\\
        Reflection7 & $15$ & $5$ & $15$ & $11$ & $0$ & $6$ & $6$ & $0.3$\\
        Reflection8 & $35$ & $7$ & $14$ & $0$ & $-21$ & $-7$ & $-28$ & $-0.67$\\
        Reflection9 & $42$ & $7$ & $21$ & $0$ & $-21$ & $-7$ & $-28$ & $-0.57$\\
        \hline
        \tsubEight{ThreadingTest}
        AsyncTask1 & $22$ & $2$ & $11$ & $1$ & $-11$ & $-1$ & $-12$ & $-0.5$\\
        Executor1 & $34$ & $7$ & $17$ & $0$ & $-17$ & $-7$ & $-24$ & $-0.59$\\
        JavaThread1 & $34$ & $7$ & $17$ & $0$ & $-17$ & $-7$ & $-24$ & $-0.59$\\
        JavaThread2 & $62$ & $10$ & $31$ & $8$ & $-31$ & $-2$ & $-33$ & $-0.46$\\
        Looper1 & $49$ & $3$ & $20$ & $16$ & $-29$ & $13$ & $-16$ & $-0.31$\\
        TimerTask1 & $203$ & $28$ & $32$ & $33$ & $-171$ & $5$ & $-166$ & $-0.72$\\
        \hhline
        \hiderowcolors
        $\varnothing$ Propagations & $85.46$ & $23.41$ & $117.64$ & $38.6$ & $32.19$ & $15.19$ & $47.37$ & $1.61$\\
        $\varnothing$ without Implicit Flow & $70.61$ & $22.46$ & $60.56$ & $40.1$ & $-10.05$ & $17.63$ & $7.59$ & $1.34$\\
        \caption{DroidBench Performance Evaluation Results}
        \label{t:droidbenchevaluation}
    \end{longtable}
    \normalsize

    \subsection{Result Explanation}
    We define tests with a relative difference greater than $10$ as worth investigating.
    In the following, we explain why our implementation performed worse than expected.

    \paragraph{PrivateDataLeak3}
    This test contains two sinks and one source.
    The tainted data is written to a file, later read from the file and then leaked.
    \textsc{FlowDroid} does not support tracking taints over files, so it only finds a leak from source to file write but misses the leak from file read to send SMS.
    Due to EasyTaintWrapper's simplicity, overtainting happens in the backward direction. When \code{FileInputStream fis = openFileInput("out.txt");} is called with \code{fis} tainted, EasyTaintWrapper also taints the base object - the \code{MainActivity} in this case.
    As the \code{MainActivity} has an enormous scope, the taint has a long lifetime and many other taints could derive from this taint.
    This taint explains the relative difference of $17.68$.
    Using the more precise SummaryTaintWrapper, the edges reduce to $(51, 16)$ and a relative difference of $2.53$, which is more reasonable.
    It is still higher because of the second sink.

    \paragraph{MultiHandlers1}
    Two \code{LocationListener}s are registered in different activities.
    In both activities, an instance field is a parameter of a sink.
    So there are two possible paths where something could be leaked.
    The LocationListener does not call any source on the first path, while the second path has an empty setter method killing the taint.
    For the first path, the backward analysis has to propagate the taint into the \code{LocationListener} to notice that this is a dead-end while the forward's search does not even start there.
    For the second path, the backward analysis seems to suffer because it starts at an instance field taint with a larger scope than a local variable.

    \paragraph{BroadcastReceiverLifecycle3}
    The test contains five sinks but only one source.
    If we only consider the leak path, both implementations perform equally.
    The four other sinks are responsible for the overhead on edge propagations.

    \paragraph{Reflection6}
    The reflective call site has multiple possible callees in the interprocedural control-flow graph.
    Backward all of these callees are visited, of which only one contains a source statement.
    Forward, the taint is introduced in the callee at the source and just one return site needs to be processed.

    \paragraph{A Note On Implicit Flows}
    All implicit flow tests and the IMEI1 test need the implicit flow rule to find the leaks.
    In those test cases our implementation does not stand a chance.
    We especially want to highlight the "every sink call influenced by conditional" semantics here.
    This semantic forces us to derive an empty taint for every conditional that is theoretically reachable from a sink.
    Beyond, we also taint the base object without any fields at every sink to detect a possible conditional object instantiation.
    Even in simple test cases such as ImplicitFlow4 this results in 10 additional taints per sink.
    Important to note is also that the prior computation of reachable conditionals is not represented in the edge propagations.
    We thus conclude that it is probably better to live without a backward-directed implicit data flow analysis.

    \subsection{Using A More Precise Taint Wrapper}
    We noticed the overtainting in \code{PrivateDataLeak3} is caused by the \code{EasyTaintWrapper}.
    Thus, we now look how using the \code{SummaryTaintWrapper} influences the edge propagations.
    The full results are in \autoref{t:droidbenchevaluation_sum}.
    In the table, we take the \code{EasyTaintWrapper} as the reference and compare it against the \code{SummaryTaintWrapper} on our implementation.
    The structure of the table is as in the last subsection.

    As we already described, \code{PrivateDataLeak3} benefits from the more precise taint wrapper.
    Similarily, many other test cases also benefit.
    Others, especially Serialization1 have more edge propagations because the \code{SummaryTaintWrapper} has a summary for a method which the \code{EasyTaintWrapper} does not handle\footnotemark{} resulting in a premature kill of a taint.
    \footnotetext{The \footnotecode{EasyTaintWrapper} contains a list of supported classes. Every method from those classes is excluded from the analysis, regardless of the method being in the list of the handled methods.}
    Even with Serialization1 included in the average, the \code{SummaryTaintWrapper} needs less total edge propagations.
    Excluding it also equals out the relative difference.
    Altogether, the \code{SummaryTaintWrapper} should be the default choice for real-world applications because it is more precise without compromising on the edge propagations.

    \footnotesize
    \begin{longtable}{l | r | r | r | r | r | r | r | r}
        & \multicolumn{2}{c|}{\textbf{EasyTW}} & \multicolumn{2}{c|}{\textbf{SummaryTW}} & \multicolumn{4}{c}{\textbf{Difference}}\\
        \multirow{-2}{*}{\textbf{Test Case}} & \textbf{\#I} & \textbf{\#A} & \textbf{\#I} & \textbf{\#A} & \textbf{\#I} & \textbf{\#A}& \textbf{Total} & \textbf{Relative}\\
        \hhline
        \endhead
        \hline
        \tsubEight{AliasingTest}
        FlowSensitivity1 & $39$ & $4$ & $71$ & $13$ & $32$ & $9$ & $41$ & $0.95$\\
        Merge1 & $89$ & $47$ & $109$ & $91$ & $20$ & $44$ & $64$ & $0.47$\\
        SimpleAliasing1 & $20$ & $3$ & $20$ & $3$ & $0$ & $0$ & $0$ & $0.0$\\
        StrongUpdate1 & $11$ & $3$ & $11$ & $3$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{AndroidSpecificTest}
        ApplicationModeling1 & $851$ & $1208$ & $427$ & $792$ & $-424$ & $-416$ & $-840$ & $-0.41$\\
        DirectLeak1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        InactiveActivity & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        Library2 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        LogNoLeak & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        Obfuscation1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Parcel1 & $86$ & $93$ & $87$ & $76$ & $1$ & $-17$ & $-16$ & $-0.09$\\
        PrivateDataLeak1 & $608$ & $766$ & $585$ & $766$ & $-23$ & $0$ & $-23$ & $-0.02$\\
        PrivateDataLeak2 & $5$ & $3$ & $5$ & $3$ & $0$ & $0$ & $0$ & $0.0$\\
        PrivateDataLeak3 & $210$ & $140$ & $38$ & $12$ & $-172$ & $-128$ & $-300$ & $-0.86$\\
        runPublicAPIField1 & $43$ & $0$ & $36$ & $0$ & $-7$ & $0$ & $-7$ & $-0.16$\\
        runPublicAPIField2 & $11$ & $0$ & $14$ & $0$ & $3$ & $0$ & $3$ & $0.27$\\
        runView1 & $69$ & $0$ & $69$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{ArrayAndListTest}
        ArrayAccess1 & $51$ & $100$ & $51$ & $100$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayAccess2 & $12$ & $0$ & $12$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayAccess3 & $51$ & $100$ & $51$ & $100$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayAccess4 & $42$ & $21$ & $42$ & $21$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayAccess5 & $34$ & $23$ & $34$ & $23$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayCopy1 & $9$ & $2$ & $9$ & $2$ & $0$ & $0$ & $0$ & $0.0$\\
        ArrayToString1 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        HashMapAccess1 & $15$ & $1$ & $15$ & $1$ & $0$ & $0$ & $0$ & $0.0$\\
        ListAccess1 & $60$ & $97$ & $77$ & $118$ & $17$ & $21$ & $38$ & $0.24$\\
        MultidimensionalArray1 & $16$ & $23$ & $16$ & $23$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{CallbackTest}
        AnonymousClass1 & $208$ & $0$ & $208$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Button1 & $43$ & $0$ & $43$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Button2 & $155$ & $254$ & $184$ & $272$ & $29$ & $18$ & $47$ & $0.11$\\
        Button3 & $109$ & $408$ & $120$ & $357$ & $11$ & $-51$ & $-40$ & $-0.08$\\
        Button4 & $43$ & $0$ & $43$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Button5 & $6$ & $3$ & $7$ & $3$ & $1$ & $0$ & $1$ & $0.11$\\
        LocationLeak1 & $260$ & $298$ & $286$ & $314$ & $26$ & $16$ & $42$ & $0.08$\\
        LocationLeak2 & $152$ & $0$ & $152$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        LocationLeak3 & $104$ & $115$ & $107$ & $115$ & $3$ & $0$ & $3$ & $0.01$\\
        MethodOverride1 & $2$ & $0$ & $2$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        MultiHandlers1 & $145$ & $149$ & $148$ & $149$ & $3$ & $0$ & $3$ & $0.01$\\
        Ordering1 & $44$ & $0$ & $44$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        RegisterGlobal1 & $49$ & $0$ & $49$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        RegisterGlobal2 & $43$ & $0$ & $43$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Unregister1 & $9$ & $0$ & $9$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{EmulatorDetectionTest}
        Battery1 & $39$ & $15$ & $39$ & $15$ & $0$ & $0$ & $0$ & $0.0$\\
        Bluetooth1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Build1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Contacts1 & $200$ & $19$ & $167$ & $4$ & $-33$ & $-15$ & $-48$ & $-0.22$\\
        ContentProvider1 & $8$ & $0$ & $8$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        DeviceId1 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        File1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        IP1 & $29$ & $0$ & $52$ & $0$ & $23$ & $0$ & $23$ & $0.79$\\
        PI1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        PlayStore1 & $8$ & $0$ & $8$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        PlayStore2 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Sensors1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        SubscriberId1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        VoiceMail1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{FieldAndObjectSensitivityTest}
        FieldSensitivity1 & $25$ & $3$ & $25$ & $3$ & $0$ & $0$ & $0$ & $0.0$\\
        FieldSensitivity2 & $19$ & $0$ & $19$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        FieldSensitivity3 & $16$ & $0$ & $16$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        FieldSensitivity4 & $8$ & $0$ & $8$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        InheritedObjects1 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ObjectSensitivity1 & $14$ & $1$ & $14$ & $1$ & $0$ & $0$ & $0$ & $0.0$\\
        ObjectSensitivity2 & $10$ & $0$ & $10$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{GeneralJavaTest}
        Clone1 & $12$ & $4$ & $19$ & $10$ & $7$ & $6$ & $13$ & $0.81$\\
        Exceptions1 & $13$ & $0$ & $13$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions2 & $13$ & $0$ & $13$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions3 & $11$ & $0$ & $11$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions4 & $22$ & $0$ & $22$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions5 & $16$ & $0$ & $16$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions6 & $23$ & $0$ & $23$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Exceptions7 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        FactoryMethods1 & $14$ & $0$ & $14$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Loop1 & $51$ & $0$ & $47$ & $0$ & $-4$ & $0$ & $-4$ & $-0.08$\\
        Loop2 & $79$ & $0$ & $75$ & $0$ & $-4$ & $0$ & $-4$ & $-0.05$\\
        Serialization1 & $22$ & $29$ & $332$ & $547$ & $310$ & $518$ & $828$ & $16.24$\\
        SourceCodeSpecific1 & $45$ & $7$ & $45$ & $7$ & $0$ & $0$ & $0$ & $0.0$\\
        StartProcessWithSecret1 & $17$ & $3$ & $18$ & $4$ & $1$ & $1$ & $2$ & $0.1$\\
        StaticInitialization1 & $9$ & $0$ & $9$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        StaticInitialization2 & $86$ & $0$ & $86$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        StaticInitialization3 & $5$ & $0$ & $5$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        StringFormatter1 & $10$ & $0$ & $10$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        StringPatternMatching1 & $8$ & $4$ & $7$ & $0$ & $-1$ & $-4$ & $-5$ & $-0.42$\\
        StringToCharArray1 & $47$ & $0$ & $43$ & $0$ & $-4$ & $0$ & $-4$ & $-0.09$\\
        StringToOutputStream1 & $25$ & $1$ & $24$ & $1$ & $-1$ & $0$ & $-1$ & $-0.04$\\
        UnreachableCode & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        VirtualDispatch1 & $88$ & $28$ & $110$ & $88$ & $22$ & $60$ & $82$ & $0.71$\\
        VirtualDispatch2 & $12$ & $0$ & $12$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        VirtualDispatch3 & $6$ & $0$ & $6$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        VirtualDispatch4 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        \hline
        \tsubEight{LifecycleTest}
        ActivityEventSequence1 & $72$ & $0$ & $73$ & $0$ & $1$ & $0$ & $1$ & $0.01$\\
        ActivityEventSequence2 & $77$ & $0$ & $77$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivityEventSequence3 & $156$ & $0$ & $156$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivityLifecycle1 & $156$ & $7$ & $156$ & $7$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivityLifecycle2 & $33$ & $0$ & $33$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivityLifecycle3 & $28$ & $0$ & $28$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivityLifecycle4 & $14$ & $0$ & $14$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ActivitySavedState1 & $7$ & $0$ & $7$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ApplicationLifecycle1 & $82$ & $0$ & $82$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ApplicationLifecycle2 & $94$ & $155$ & $94$ & $155$ & $0$ & $0$ & $0$ & $0.0$\\
        ApplicationLifecycle3 & $21$ & $0$ & $21$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        AsynchronousEventOrdering1 & $16$ & $0$ & $16$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        BroadcastReceiverLifecycle1 & $4$ & $0$ & $4$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        BroadcastReceiverLifecycle2 & $248$ & $114$ & $208$ & $98$ & $-40$ & $-16$ & $-56$ & $-0.15$\\
        BroadcastReceiverLifecycle3 & $195$ & $110$ & $144$ & $82$ & $-51$ & $-28$ & $-79$ & $-0.26$\\
        EventOrdering1 & $30$ & $0$ & $30$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        FragmentLifecycle1 & $90$ & $0$ & $90$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        FragmentLifecycle2 & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$ & $-$\\
        ServiceEventSequence1 & $124$ & $34$ & $122$ & $38$ & $-2$ & $4$ & $2$ & $0.01$\\
        ServiceEventSequence2 & $389$ & $220$ & $315$ & $176$ & $-74$ & $-44$ & $-118$ & $-0.19$\\
        ServiceEventSequence3 & $275$ & $151$ & $232$ & $110$ & $-43$ & $-41$ & $-84$ & $-0.2$\\
        ServiceLifecycle1 & $42$ & $0$ & $42$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        ServiceLifecycle2 & $89$ & $21$ & $89$ & $21$ & $0$ & $0$ & $0$ & $0.0$\\
        SharedPreferenceChanged1 & $11$ & $0$ & $8$ & $0$ & $-3$ & $0$ & $-3$ & $-0.27$\\
        \hline
        \tsubEight{ReflectionTest}
        Reflection1 & $8$ & $0$ & $8$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Reflection2 & $11$ & $0$ & $11$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Reflection3 & $62$ & $25$ & $50$ & $0$ & $-12$ & $-25$ & $-37$ & $-0.43$\\
        Reflection4 & $8$ & $0$ & $8$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Reflection5 & $11$ & $0$ & $11$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Reflection6 & $134$ & $51$ & $122$ & $31$ & $-12$ & $-20$ & $-32$ & $-0.17$\\
        Reflection7 & $15$ & $11$ & $3$ & $0$ & $-12$ & $-11$ & $-23$ & $-0.88$\\
        Reflection8 & $14$ & $0$ & $14$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        Reflection9 & $21$ & $0$ & $21$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        \hline
        \tsubEight{ThreadingTest}
        AsyncTask1 & $11$ & $1$ & $11$ & $1$ & $0$ & $0$ & $0$ & $0.0$\\
        Executor1 & $17$ & $0$ & $17$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        JavaThread1 & $17$ & $0$ & $17$ & $0$ & $0$ & $0$ & $0$ & $0.0$\\
        JavaThread2 & $31$ & $8$ & $28$ & $8$ & $-3$ & $0$ & $-3$ & $-0.08$\\
        Looper1 & $20$ & $16$ & $20$ & $16$ & $0$ & $0$ & $0$ & $0.0$\\
        TimerTask1 & $32$ & $33$ & $45$ & $37$ & $13$ & $4$ & $17$ & $0.26$\\
        \hhline
        $\varnothing$ Propagations & $60.56$ & $40.1$ & $57.29$ & $39.16$ & $-3.27$ & $-0.93$ & $-4.2$ & $0.13$\\
        $\varnothing$ without Serialization1& $60.88$ & $40.19$ & $55.04$ & $35.0$ & $-5.84$ & $-5.19$ & $-11.02$ & $0.0$\\
        \caption{\textsc{DroidBench} Evaluation with Summary Taint Wrapper}
        \label{t:droidbenchevaluation_sum}
    \end{longtable}
    \normalsize

    \section{Real World Apps}\label{s:realworld}

    \subsection{Configuration}
    Our test machine is equipped with four Intel Xeon E5-4650 and 1 TB of RAM.
    We limited the JVM to 50 GB RAM and \textsc{FlowDroid} on 16 threads per instance.
    We ran at most four instances in parallel to ensure a one-to-one mapping between CPU threads and \textsc{FlowDroid} threads.
    Note that the test machine is a shared system, but we made sure there are always enough resources for our evaluation available.
    Still, background services might influence the performance of a single run. To stamp out this factor, we ran each app three times with a distance of time\footnotemark.
    \footnotetext{The time distance between each run is at least the elapsed time from the analysis of the remaining 199 apps.}
    If there were outliers\footnotemark{}, we repeated the run.
    \footnote{Outliers are runs with at least $25\%$ difference to the median run and a minimum of $5$ seconds absolute difference.}
    Some runs did not comply to our outlier norm even after we ran them multiple times, but this only concers 9 out of 600 runs.

    We also measured the memory usage of both implementations.
    Using the memory amount reported by the JVM is not precise because the JVM prefers to take up free memory before running the garbage collector \cite{Arzt2017PhD}.
    We borrowed the memory evaluation tool from CleanDroid, which internally depends on a memory calculation tool from Twitter\footnote{\url{https://mvnrepository.com/artifact/com.twitter.common/objectsize} (visited on 18.04.2021)}.
    The memory evaluation tool measures the size of the exploded supergraph in 15 seconds intervals \cite{Arzt2021}.
    Because we do not want to pollute the measured data flow time with the memory evaluation tool's latency, the memory measuring runs were run independently of the time measuring runs.
    The memory sampling also takes up memory and because our test system has enough memory available, we bumped the maximum heap size up to 100GB, effectively eliminating memory timeouts.

    For this evaluation, we chose to use a non-default configuration of \textsc{FlowDroid}.
    First, we disabled static field tracking due to the global scope as described in \autoref{s:complexity}.
    Next, instead of the \code{EasyTaintWrapper}, we use the \code{SummaryTaintWrapper}, which utilizes StubDroid.
    We set the timeout for the data flow analysis to 10 minutes\footnotemark{}.
    \footnotetext{A timeout in FlowDroid prevents processing new edges but lets the solver finish the current edge propagation. Thus, some apps may have a data flow time of above 600 seconds.}
    The call graph generation was limited to 180 seconds and the call-graphs were serialized before, so every run was on the same call-graph.
    The configuration summary is in \autoref{t:realworldconfig}.

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | l}
            \textbf{Option} & \textbf{Value}\\
            \hline\hline
            Array Size Tainting & disabled\\
            Inspect Sources \& Sinks & disabled\\
            Static Field Tracking & disabled\\
            Ignore Flows in System Packages & enabled\\
            Exclude Soot Library Classes & enabled\\
            Timeout & 10 minutes\\
            Taint Wrapper & \code{SummaryTaintWrapper}\\
        \end{tabular}
        \caption{Real World Apps Configuration}
        \label{t:realworldconfig}
    \end{table}

    We did not use the full sources and sinks list included in \textsc{FlowDroid} because such would result in hundreds of sources and sinks per app and probably a long runtime.
    Instead, we chose to analyze which sensitive and possibly user-identifying data is sent out to the internet.
    As we want to compare the forwards and backward implementation, it is also essential to not put one at a disadvantage.
    We opted for a 2:1 ratio of sources to sinks.
    This decision is based on the results of SuSi, to find sources and sinks in the Android framework automatically \cite{Rasthofer2014}.
    Their extracted list of sources and sinks contains roughly $2.17$ times more sources than sinks.
    The list of sources and sinks used in this evaluation is in \autoref{t:realworldsources} and \autoref{t:realworldsinks}.

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | l}
            \textbf{Class} & \textbf{Method}\\
            \hline\hline
            android.location.Location & getLatitude()\\
            & getLongitude()\\
            \hline
            android.location.LocationManager & getLastKnownLocation()\\
            \hline
            android.telephony.TelephonyManager & getDeviceId()\\
            & getSubscriberId()\\
            & getSimSerialNumber()\\
            & getLine1Number()\\
            & getImei()\\
            & getMeid()\\
            \hline
            android.bluetooth.BluetoothAdapter & getAddress()\\
            android.net.wifi.WifiInfo & getMacAddress()\\
            & getSSID()\\
            & getIpAddress()\\
            \hline
            java.net.InetAddress & getHostAddress()\\
            \hline
            android.telephony.gsm.GsmCellLocation & getCid()\\
            & getLac()\\
            \hline
            android.content.pm.PackageManager & getInstalledApplications()\\
            & getInstalledPackages()\\
            & queryIntentActivities()\\
            & queryIntentServices()\\
            & queryBroadcastReceivers()\\
            \hline
            android.content.SharedPreferences & getDefaultSharedPreferences()\\
            android.provider.Browser & getAllBookmarks()\\
            & getAllVisitedUrls\\
        \end{tabular}
        \caption{Sources for Real World Apps Evaluation}
        \label{t:realworldsources}
    \end{table}

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | l}
            \textbf{Class} & \textbf{Method}\\
            \hline\hline
            java.net.URL & set()\\
            & openConnection()\\
            \hline
            java.net.URLConnection & connect()\\
            & setRequestProperty()\\
            \hline
            android.net.http.HttpsConnection & openConnection()\\
            \hline
            android.net.http.Headers & setEtag()\\
            & setContentType()\\
            & setLastModified()\\
            & setLocation()\\
            \hline
            android.net.http.AndroidHttpClientConnection & sendRequestHeader()\\
            \hline
            android.net.http.RequestQueue & queueRequest()\\
        \end{tabular}
        \caption{Sinks for Real World Apps Evaluation}
        \label{t:realworldsinks}
    \end{table}

    We used \textsc{FlowDroid}'s forward implementation on the to that date latest upstream commit\footnote{The latest upstream commit was at that time \footnotecode{b436733fc4a5130dfe4ce8ddb3f76fd374e9a487}.} from the develop branch for the point of comparison.
    The backward implementation ran on our latest commit\footnotemark{} at that time with all changes from the upstream merged into.
    \footnotetext{Our latest commit was \footnotecode{87bf33ba40ef8b4fb25f33439d887ebc98c2c184}. Note that during the real-world evaluation we found some bugs and also later on fixed some edge cases in the analysis. All fixes should not influence the runtime in a bad way.}

    We chose 200 apps randomly out of a Google Playstore dump from 2021 containing over 6000 apps for our evaluation set.
    Out of 200 apps, 60 apps do not have any sources or sinks and thus, the analysis did not start.
    For six apps, the analysis aborted with errors on at least one run. All thrown exceptions happened outside of \textsc{FlowDroid}.
    We are left with 131 apps for which both implementations completed all runs without errors.
    The full list is appended to this work in \autoref{a:appset}.

    \FloatBarrier
    \subsection{Time Evaluation}
    In general, the individual apps' runtimes were far apart from each other.
    We had many apps with a single-digit analysis time and on the other side, we also found many apps that triggered a timeout or were close to triggering one.
    In between those extrema are only a few apps.
    Recall that we set a soft limit on the runtime at 600 seconds.
    The reference forward runs have a standard deviation of $209$ seconds and the runs of our implementation has $277$ seconds standard deviation.
    It is important to keep this in mind when interpreting the results.

    We first begin with an overview of the results.
    \autoref{t:realworldresults} shows the results, including timeouts.
    Notably, the backward analysis had $20\%$ less time timeouts than the forward analysis.
    In return, it seems a bit more memory-hungry with $3.63\%$ more memory timeouts.
    We conducted a t-test to check the significance of those differences with the null hypothesis of equal average expected values.
    The p-value for the memory timeouts is $0.156$, thus being insignificant.
    A t-test over the runtime yielded a p-value of $0.00036$, meaning the advantage for our implementation is significant.
    We cover the memory consumption extensively in the \hyperref[s:memex]{next subsection} and focus on the time for now.
    Interestingly, the propagated edges along the same interprocedural call-graph are of the same order of magnitude.
    Also, the 85\textsuperscript{th} percentile runtime is nearly equal and the median is equal.
    However, claims based on the runtime and edges with timeouts are only possible to a limited extent because the timeout highly influences both values.

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | r | r | r}
            & \multicolumn{3}{c}{\textbf{Forward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Data Flow Time & $518.93s$ & $600.00s$ & $605.10s$\\
            \hline
            Edge Propagations Infoflow & $34555326.97$ & $41743088.00$ & $52163969.60$\\
            Edge Propagations Alias & $12562479.07$ & $14598571.50$ & $18638900.10$\\
            Total Edge Propagations & $47117806.04$ & $57697027.00$ & $70602469.30$\\
            \hline
            Memory Timeouts & \multicolumn{3}{r}{$2.99\%$}\\
            Time Timeouts & \multicolumn{3}{r}{$80.60\%$}\\
            \multicolumn{4}{c}{}\\
            & \multicolumn{3}{c}{\textbf{Backward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Data Flow Time & $413.19s$ & $600.00s$ & $606.00s$\\
            \hline
            Edge Propagations Infoflow & $13826566.90$ & $14981108.50$ & $23712802.00$\\
            Edge Propagations Alias & $33567561.46$ & $43444060.00$ & $56773141.00$\\
            Total Edge Propagations & $47394128.36$ & $60855935.50$ & $79405729.00$\\
            \hline
            Memory Timeouts & \multicolumn{3}{r}{$6.62\%$}\\
            Time Timeouts & \multicolumn{3}{r}{$59.56\%$}\\
        \end{tabular}
        \caption{Results With Timeouts}
        \label{t:realworldresults}
    \end{table}

    Next, we only consider the runs without any timeouts in \autoref{t:realworldresultswithouttimeout}.
    This time we can still observe a relation between backward infoflow edges and forward alias edges even though to a lesser extent.
    More significant, backward needed way less forward propagations either because fewer aliases were on the path or the alias analysis could be stopped earlier due to a near turn unit.
    The runtimes also represent this fact. In the 85\textsuperscript{th} percentile, both analyses are more close than the average suggest, with the backward analysis needing $2.25$ seconds less.
    The median here renders useless as a comparison point because of the huge variance in the data set.

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | r | r | r}
            & \multicolumn{3}{c}{\textbf{Forward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Data Flow Time & $76.91s$ & $0.00s$ & $108.05s$\\
            \hline
            Edge Propagations Infoflow & $5651572.23$ & $28692.00$ & $7164554.30$\\
            Edge Propagations Alias & $1864306.77$ & $8689.50$ & $4212872.40$\\
            Total Edge Propagations & $7515879.00$ & $37310.00$ & $11377426.70$\\
            \multicolumn{4}{c}{}\\
            & \multicolumn{3}{c}{\textbf{Backward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Data Flow Time & $35.20s$ & $0.00s$ & $39.25s$\\
            \hline
            Edge Propagations Infoflow & $2353723.61$ & $19940.50$ & $1733519.75$\\
            Edge Propagations Alias & $3795159.13$ & $20013.50$ & $6883734.25$\\
            Total Edge Propagations & $6148882.74$ & $39954.00$ & $8692988.50$\\
        \end{tabular}
        \caption{Results without Timeouts}
        \label{t:realworldresultswithouttimeout}
    \end{table}

    A knowledgeable reader might have noticed the results in \autoref{t:realworldresults} are worse than in previous publications where \textsc{FlowDroid} was evaluated \cite{Arzt2017PhD, Arzt2021}.
    We want to emphasize that none of our changes did influence the reference runs in a bad way as we used the upstream version without a single line changed to conduct the first run\footnotemark{}.
    \footnotetext{We found some exceptions in the upstream project while evaluating, so after the first run we switched to our version with the fixes included. This did not change the results we got.}
    The existing implementation suffers as ours, so we suspect it partly depends on an unfortunatly drawn app set and further development in the call-graph generation leading to more possible edges.

    With that out of the way, let us look at the results in greater detail. We now compare the analysis on a per-app basis.
    The histogram is in \autoref{f:deltaHist}.
    We compiled the delta data flow time of the analyses per app, calculated as in the last section with the forward implementation being the reference: $t_{\mathit{Backward}} - t_{\mathit{Forward}}$.
    Hence, negative values represent that our implementation performed better.
    The delta on the x-axis is given in seconds and the frequency on the y-axis in number of apps.
    The bins always span over $50$ seconds.
    The graph shows a large number of apps around $0$ with a slight bias towards the forward implementation.
    Equivalent to the distribution of the data flow times, there are only few deltas in the range from $\pm100$ to $\pm500$.
    More interestingly, there are significantly more apps around $-600$ than around $600$.
    Recall, the timeout is set to $600s$.
    So, our implementation terminates nearly instantaneous in some cases on which the forward analysis times out.
    As expected, there is no general advantage for a direction.
    Instead, we observe a per-app advantage in around $60\%$ of the test set, while for the rest, the performance is similar.

    \begin{figure}[tbp]
        \centering
        \resizebox{0.75\columnwidth}{!}{
            \input{figs/deltaHist.pgf}
        }
        \caption{Histogram of the Delta Data Flow Time}
        \label{f:deltaHist}
    \end{figure}

    We confirmed that the right direction choice can speed up the analysis by a magnificent amount. To take advantage of the favorable direction, we now investigate the correlating conditions for the advantageous direction.
    Most straightforward would be a correlation between the difference of source and sink count and the data flow time.
    In \autoref{f:dfratio} are two graphs with the ratio of sources and sinks ($\frac{\mathit{Sinks} - \mathit{Sources}}{\mathit{Sources}}$) on the x-axis and the data flow time in seconds on the y-axis.
    The left graph is always the forward implementation and the right graph is our implementation.
    Blue dots represent apps without a timeout, orange a time timeout and red a memory timeout.
    Intuitively, a negative ratio should put our implementation at an advantage. The graphs show no correlation between the ratio and the runtime, neither forward nor backward.
    We also included the forward data flow time by sources and the backward data flow time by sinks in \autoref{f:dfsources} and \autoref{f:dfsinks}.
    The number of sinks backward and the number of sources forward do not influence the runtime.
    So we can confirm Arzt's evaluation\cite{Arzt2017PhD} as there is no correlation between sources and the forward runtime in our app set.
    Parallel to this observation, the sink count does not influence the backward runtime.
    The sink count for forward and the source count for backward can not influence the runtime they have no influence on the edge propagations.


    \begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \begin{subfigure}[]{\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-sources.pgf}
                }
            \end{subfigure}
            \caption{Source Count}
            \label{f:dfsources}
        \end{subfigure}
        \qquad
        \begin{subfigure}[b]{0.45\textwidth}
            \centering
            \begin{subfigure}[]{\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-sinks.pgf}
                }
            \end{subfigure}
            \caption{Sink Count}
            \label{f:dfsinks}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-ssratio.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-ssratio.pgf}
                }
            \end{subfigure}
            \caption{Ratio}
            \label{f:dfratio}
        \end{subfigure}
        \caption{Data Flow Time in Comparison to Sources, Sinks and the Ratio of Those}
        \label{f:dftoss}
    \end{figure}

    Even though Arzt's evaluation also showed no correlation between the code size \cite{Arzt2017PhD}, we do for completeness also compare the runtime to the number of statements, methods and classes.
    Note that these refer to the Jimple intermediate representation and not Java.
    \autoref{f:dftocodesize} includes all graphs with the existing implementation being on the left side and our implementation on the right side.
    The notation are the same as before, with the x-axis swapped out.
    The number of statements is uniformly distributed with some outliers.
    If we consider our data as two linear data sets with a structural break between the two groups, the linear regressions have a slope of close to 0.
    Resulting, the number of statements, methods and classes do not have an impact on the runtime.

    \begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-statements.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-statements.pgf}
                }
            \end{subfigure}
            \caption{Statements}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-methods.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-methods.pgf}
                }
            \end{subfigure}
            \caption{Methods}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-classes.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-classes.pgf}
                }
            \end{subfigure}
            \caption{Classes}
        \end{subfigure}
        \caption{Data Flow Time in Comparison to Code Size}
        \label{f:dftocodesize}
    \end{figure}

    Because the above mentioned parameters do not influence the runtime, we did further investigate to find a parameter to decide the favorable direction.
    First, we looked the methods containing sources and sinks.
    We counted the number of statements of the method, call statements and callers of the method and compared these numbers between sources and sinks.
    A advantage in those did not result in a faster analysis.
    Next, we implemented a fast intraprocedural taint analysis. It omits access paths and aliasing.
    Method calls are overapproximated in a similar fashion to the \code{EasyTaintWrapper}.
    We then counted the taints flowing into the callees and callers.
    Also, we did count the number of taints in the method.
    The drawback is that this only works when the state explosion happens inside the first method and this is not the case in the app set.
    Again, we could not find any resilient correlation.
    At this point, we run out of easily computable facts about an app that could correlate with the runtime and decided to leave the question up for future work.

    Finally, we compare the number of edges in the exploded supergraph, referred to as taint propagations in \autoref{s:complexity}.
    Note that the edges in the exploded supergraph are only known after the analysis, making them useless for predictions.
    In \autoref{f:dfedgestotal} we plot the edge count on the x-axis to the data flow time on the y-axis.
    In both graphs, we observe a linear correlation for the apps with a runtime below 500 seconds.
    Then there is a structural break and after that the apps time out.
    Because a linear regression does not really fit well for our diverse data set, we decided to fit a function using the least squares method. We achieved a $r^2$ measure of greater than $0.9$ for four degree polynomials and above.
    However, the good fitting curve seems overfitted to us because the apps not being close to timeouts have a good fitting linear correlation.
    When we look at the point where the structural break happens, we notice that backward the timeouts start after roughly $3 \cdot 10^7$ propagations.
    Forward on the other hand only gets to around $2 \cdot 10^7$ edge propagations before reaching a timeout.
    Such a large difference is unintuitive because the computation cost should not be much different.
    We split the edges up by IFDS problems in \autoref{f:dfedgesi} and \autoref{f:dfedgesa}.
    Interestingly, all curves have a similar steep curve with the exception of the backward alias analysis being more shallow.
    This gives a possible explanation linked to the ratio of infoflow and alias edges.
    The alias flow functions are way simpler and thus, should also cost less to compute.
    The backward analysis has a ratio biased toward the alias edges which could explain the higher edge count possible in ten minutes.
    Why the structural break happens could not be conclusively clarified in this work, so it is also hard to finally reason whether this also holds without such a structural break.

    \begin{figure}[tbp]
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-edges.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-edges.pgf}
                }
            \end{subfigure}
            \caption{Total Edges}
            \label{f:dfedgestotal}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-iedges.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-iedges.pgf}
                }
            \end{subfigure}
            \caption{Infoflow Edges}
            \label{f:dfedgesi}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_dataflow-aedges.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_dataflow-aedges.pgf}
                }
            \end{subfigure}
            \caption{Alias Edges}
            \label{f:dfedgesa}
        \end{subfigure}
        \caption{Data Flow Time in Comparison to Edge Count}
        \label{f:dfedges}
    \end{figure}

    To conclude, our backward analysis is efficient enough to be an alternative to the existing implementation.
    We even found that it performed slightly better on our app set.
    Our evaluation shows that there is no correlation between an apriori known parameter and the runtime of \textsc{FlowDroid} - in both directions.
    Furthermore, we did not find any apriori known parameter to decide the favorable direction either.
    The edge propagations have shown that our implementation can analyze roughly $10^7$ more edges than the existing implementation in ten minutes. Though, the sample size of 200 apps is too small to generalize statements and our data was rather challenging to interpret with a large standard deviation.

    \FloatBarrier
    \subsection{Memory Evaluation}\label{s:memex}

    \begin{table}[tbp]
        \centering
        \begin{tabular}{l | r | r | r}
            & \multicolumn{3}{c}{\textbf{Forward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Maximum Memory Consumption & $10005.68MB$ & $10459.48MB$ & $15482.98MB$\\
            \hline
            Maximum Memory Consumption & \multirow{2}{*}{$1535.20MB$} & \multirow{2}{*}{$6.98MB$} & \multirow{2}{*}{$4952.20MB$}\\
            Without Timeouts & & &\\            
            \multicolumn{4}{c}{}\\
            & \multicolumn{3}{c}{\textbf{Backward}}\\
            \textbf{Metric} & \textbf{Avg} & \textbf{Median} & $\mathbf{P_{85}}$\\
            \hline\hline
            Maximum Memory Consumption & $8326.27MB$ & $10008.52MB$ & $14539.64MB$\\
            \hline
            Maximum Memory Consumption & \multirow{2}{*}{$1473.21MB$} & \multirow{2}{*}{$7.33MB$} & \multirow{2}{*}{$1566.61MB$}\\
            Without Timeouts & & &\\
        \end{tabular}
        \caption{Memory Results}
        \label{t:memres}
    \end{table}

    \autoref{t:memres} shows an overview of the results from the memory evaluation.
    Note that we only measured the memory usage of the edges in the exploded supergraph and not of the full program.
    Also, unlike the time measurements, the memory consumption is much more distributed distributed across the range.
    The measurements with timeouts show similar values for both directions with a bias toward our implementation.
    Though, maximum measurements with timeouts are not really meaningful because of the cut-off at ten minutes.
    Without timeouts the gap gets a bit bigger.
    The average maximum memory consumption of our implementation is around $65MB$ lower than the existing one. 
    Especially in the 85\textsuperscript{th} percentile our implementation shines where it needs $3.3GB$ less memory.

    Next, we look at the memory consumption difference per app in \autoref{f:memHist}.
    The x-axis shows the delta maximum memory consumption in megabytes and the y-axis the frequency.
    Each bin is $1GB$ wide.
    The delta is calculated with forward as the refrence: $m_{\mathit{Backward}}-m_{\mathit{Forward}}$.
    Again, we see a gathering around $0$. 
    Otherwise, the histogram has a more uniform distribution than its \hyperref[f:deltaHist]{time counterpart}.
    Just as in the overview, there is a slight bias towards the backward analysis.
    We argue this bias is related to the faster backward analysis in the app set. 
    A faster analysis means less edges\footnotemark{} and the edges should correlate linearly with the memory usage of the exploded supergraph.
    \footnotetext{This was at least true for apps without timeout.} 
    We looked at this by comparing the sign of the delta data flow time with the sign of the delta memory consumption. 
    48 apps had different signs, with 23 being negligibly close to 0. 
    Hence, the claim is true for 109 of 134 apps.
    We also calculated the Pearson correlation coefficient for the data flow time and the maximum memory consumption.
    For the forward analysis the coefficient is $0.75$ and $0.87$ for the backward implementation.
    Both values indicate a correlation.

    \begin{figure}[tbp]
        \centering
        \resizebox{0.75\columnwidth}{!}{
            \input{figs/memHist.pgf}
        }
        \caption{Histogram of the Delta Maximum Memory Consumption}
        \label{f:memHist}
    \end{figure}

    For completeness, we now validate our claim that the edges in the exploded supergraph correlate linearly with the memory consumption.
    We expect a linear correlation because the exploded supergraph is represented as a \code{HashMap} of edges and taints. 
    Consider \autoref{f:maxmemedges} where a clear, linear correlation is visible.
    We confirmed our claim.

    \begin{figure}[tbp]
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-edges.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-edges.pgf}
                }
            \end{subfigure}
        \end{subfigure}
        \caption{Maximum Memory Consumption in Comparison to the Edge Count}
        \label{f:maxmemedges}
    \end{figure}

    Also beneficial for the real-world usage of \textsc{FlowDroid} would be to estimate the memory consumption to utilize the available resources efficiently.
    In \autoref{f:maxmemtoss}, we contrast the memory consumption with the number of sources, sinks and the ratio of both.
    \autoref{f:maxmemtocodesize} shows the memory consumption in contrast to the statement, method and class count.
    The arrangement and legend are the same as in the time evaluation.
    Unlike in the time evaluation, there is only one cluster of dots: those terminating nearly instantaneous. 
    Otherwise, the dots seem to be randomly distributed. 
    All graphs indicate no correlation.

    \begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-sources.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-sources.pgf}
                }
            \end{subfigure}
            \caption{Sources}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-sinks.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-sinks.pgf}
                }
            \end{subfigure}
            \caption{Sinks}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-ssratio.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-ssratio.pgf}
                }
            \end{subfigure}
            \caption{Ratio}
        \end{subfigure}
        \caption{Maximum Memory Consumption In Comparison To Source, Sink And Edge Count}
        \label{f:maxmemtoss}
    \end{figure}

    \begin{figure}[tbp]
        \centering
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-statements.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-statements.pgf}
                }
            \end{subfigure}
            \caption{Statements}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-methods.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-methods.pgf}
                }
            \end{subfigure}
            \caption{Methods}
        \end{subfigure}
        \bigbreak
        \begin{subfigure}[b]{\textwidth}
            \centering
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/fw_maxmem-classes.pgf}
                }
            \end{subfigure}
            \qquad
            \begin{subfigure}[]{0.45\textwidth}
                \centering
                \resizebox{\columnwidth}{!}{
                    \input{figs/bw_maxmem-classes.pgf}
                }
            \end{subfigure}
            \caption{Classes}
        \end{subfigure}
        \caption{Maximum Memory Consumption in Comparison to Code Size}
        \label{f:maxmemtocodesize}
    \end{figure}

    To conclude, our backward analysis performed a bit better in the time evaluation, which is also reflected in the memory consumption. Again, the results show that the observed edges are way more important for memory consumption than the code size or the sources and sinks. It is not possible to estimate the memory consumption prior nor which analysis direction will use less memory.

    % Basically the answer to RQ1: Is the backwards search efficient enough to perform analysis on real world apps?

    % Basically the answer to RQ2: Can we find a pre-analysis known parameter to decide which analysis is more efficient?
    \FloatBarrier
\end{document}